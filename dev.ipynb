{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete!\n",
      "Found 1 duplicates:\n",
      "Removed duplicate: 352. **police** - Police\n",
      "\n",
      "Cleaned file saved as: cleaned_vocabulary.txt\n",
      "Total entries in cleaned file: 424\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def clean_vocabulary_file(input_file, output_file):\n",
    "    try:\n",
    "        # Read the input file\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "\n",
    "        # Split content into sections based on headers (marked with **)\n",
    "        sections = re.split(r'\\n\\*\\*[^*]+\\*\\*\\n', content)\n",
    "        headers = re.findall(r'\\*\\*([^*]+)\\*\\*', content)\n",
    "        \n",
    "        # Dictionary to store words and their first occurrences\n",
    "        word_dict = defaultdict(list)\n",
    "        seen_translations = set()\n",
    "        duplicates_found = []\n",
    "        \n",
    "        # Process the cleaned content\n",
    "        cleaned_sections = []\n",
    "        current_line_number = 1\n",
    "        \n",
    "        # Process the header section separately if it exists\n",
    "        if sections[0].strip() == '':\n",
    "            sections = sections[1:]\n",
    "        \n",
    "        # Process each section\n",
    "        for section_idx, section in enumerate(sections):\n",
    "            cleaned_lines = []\n",
    "            lines = section.strip().split('\\n')\n",
    "            \n",
    "            for line in lines:\n",
    "                if line.strip():\n",
    "                    # Extract number and content using regex\n",
    "                    match = re.match(r'(\\d+)\\. \\*\\*([^*]+)\\*\\* - (.+)', line)\n",
    "                    if match:\n",
    "                        number, word, translation = match.groups()\n",
    "                        \n",
    "                        # Create a key that combines the word and translation\n",
    "                        entry_key = f\"{word.lower().strip()} - {translation.lower().strip()}\"\n",
    "                        \n",
    "                        if entry_key not in seen_translations:\n",
    "                            seen_translations.add(entry_key)\n",
    "                            cleaned_lines.append(line)\n",
    "                        else:\n",
    "                            duplicates_found.append((word, translation, number))\n",
    "            \n",
    "            if cleaned_lines:\n",
    "                # Add header back\n",
    "                if section_idx < len(headers):\n",
    "                    cleaned_sections.append(f\"\\n**{headers[section_idx]}**\\n\")\n",
    "                cleaned_sections.append('\\n'.join(cleaned_lines))\n",
    "        \n",
    "        # Renumber all entries\n",
    "        final_content = cleaned_sections[0]  # First header\n",
    "        current_number = 1\n",
    "        \n",
    "        for section in cleaned_sections[1:]:\n",
    "            lines = section.split('\\n')\n",
    "            renumbered_lines = []\n",
    "            for line in lines:\n",
    "                if line.startswith('**'):\n",
    "                    renumbered_lines.append(line)\n",
    "                elif line.strip():\n",
    "                    new_line = re.sub(r'^\\d+', str(current_number), line)\n",
    "                    renumbered_lines.append(new_line)\n",
    "                    current_number += 1\n",
    "            final_content += '\\n' + '\\n'.join(renumbered_lines)\n",
    "        \n",
    "        # Write the cleaned content to output file\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(final_content)\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"Processing complete!\")\n",
    "        print(f\"Found {len(duplicates_found)} duplicates:\")\n",
    "        for word, translation, number in duplicates_found:\n",
    "            print(f\"Removed duplicate: {number}. **{word}** - {translation}\")\n",
    "        print(f\"\\nCleaned file saved as: {output_file}\")\n",
    "        print(f\"Total entries in cleaned file: {current_number - 1}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "# Example usage\n",
    "input_file = \"Thanglish.txt\"  # Your input file name\n",
    "output_file = \"cleaned_vocabulary.txt\"  # Your output file name\n",
    "clean_vocabulary_file(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv(\"tanglish_vocabulary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_words = df['tanglish'].unique()\n",
    "pd.DataFrame({'tanglish': uni_words}).to_csv('thanglish_words.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'your_file.csv' with the path to your CSV file\n",
    "df = pd.read_csv(\"phrases.csv\", header=None)\n",
    "# Flatten the DataFrame into a single column and remove NaN values\n",
    "df_flat = df.values.flatten()\n",
    "df_flat = pd.Series(df_flat).dropna().reset_index(drop=True)\n",
    "\n",
    "# Convert to DataFrame with 'phrases' as the column name\n",
    "df_phrases = pd.DataFrame(df_flat, columns=['phrases'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_phrases.to_csv('phrases_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Naan kadaikku vengayam vaangaren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 phrases\n",
       "count                                100\n",
       "unique                               100\n",
       "top     Naan kadaikku vengayam vaangaren\n",
       "freq                                   1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_phrases.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv(\"phrases_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "# Read the ground_truth.csv file\n",
    "ground_truth_df = pd.read_csv(\"ground_truth.csv\", header=None, names=['phrases'])\n",
    "\n",
    "# Read the tamil_dictionary.csv file\n",
    "tamil_dict_df = pd.read_csv(\"tamil_dictionary.csv\", header=None, names=['tanglish'])\n",
    "\n",
    "# Extract unique words from ground_truth_df\n",
    "ground_truth_words = set(ground_truth_df['phrases'].str.split(expand=True).stack().unique())\n",
    "\n",
    "# Extract words from tamil_dict_df\n",
    "tamil_dict_words = set(tamil_dict_df['tanglish'].unique())\n",
    "\n",
    "# Find words in ground_truth that are not in tamil_dictionary\n",
    "missing_words = ground_truth_words - tamil_dict_words\n",
    "\n",
    "# Add missing words to the tamil_dictionary DataFrame\n",
    "for word in missing_words:\n",
    "    tamil_dict_df = tamil_dict_df.append({'tanglish': word}, ignore_index=True)\n",
    "\n",
    "# Save the updated tamil_dictionary.csv\n",
    "tamil_dict_df.to_csv(\"tamil_dictionary.csv\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
